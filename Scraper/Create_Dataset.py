# -*- coding: utf-8 -*-
"""Traffic Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wKOfgcnqp1K-Gz2CE6sgTteOujRX-Nro
"""

import urllib.request as urllib2
import json
import pandas as pd
import time
import datetime

"""
INPUTS:
    url: a request url
OUTPUTS: 
    the data returned by calling that url
"""
def request_data_from_url(url):
    req = urllib2.Request(url)
    success = False
    while success is False:
        try: 
            
            response = urllib2.urlopen(req)
            
            if response.getcode() == 200:
                success = True
        except Exception:
            #if we didn't get a success, then print the error and wait 5 seconds before trying again
            time.sleep(5)

            print("Error for URL %s: %s" % (url, datetime.datetime.now()))
            print("Retrying...")

    #return the contents of the response
    return response.read()

column=['Source','Destination','TimeofDay','Time_Taken']

keys="" #TYPE API KEY HERE

"""
INPUTS:
    api_key: authentication to GMaps that we're allowed to request this data
    origin: lat,long of origin
    destination: lat,long of destination
    frequency: how often to scrape the data
    duration: how long to scrape the data
OUTPUTS:
    Return Pandas DF
"""
def scrape_gmaps_data(api_key, origins, destinations, frequency, duration):
    df=pd.DataFrame(columns=column)
    #we want to scrape the googlemaps website
    site = 'https://maps.googleapis.com/maps/api/'
    #we want to use the distancematrix service
    service = 'distancematrix/json?'
    #API key from user
    key = 'key=%s' % (api_key)
    step=0
    while (step <= int(duration*60 / frequency)):
      
      for origin,destination in zip(origins,destinations):
        #input origin and destination from the user 
        locations = 'origins=%s&destinations=%s&departure_time=now&' % (origin, destination)
        
        #construct request url
        request_url = site + service + locations + key
        data = json.loads(request_data_from_url(request_url))
        df=df.append({column[0]:data['origin_addresses'][0],column[1]:data['destination_addresses'][0],column[2]:datetime.datetime.now(),column[3]:data['rows'][0]['elements'][0]['duration_in_traffic']['value']},ignore_index=True)
        
      if step % 10 == 0:
        print(str(step) + ' datapoints gathered ...')
      df.to_csv('Traffic_Dataset.csv',index=False)
      step += 1
      time.sleep(frequency*60) 
    return df

api_key = keys
origin = ['28.750616,77.116578','30.727546,76.844814','30.752827,76.806375'] #DTU , Infosys , Rock Garden
destination = ['30.727546,76.844814','30.752827,76.806375','28.750616,77.116578'] #INFOSYS, Rock Garden , DTU
frequency = int(input('How Often to Scrape Data (minutes): '))
duration = int(input('How Long to Scrape Data (hours): '))

if __name__ == '__main__':
    df=scrape_gmaps_data(api_key, origin, destination, frequency, duration)
    df.to_csv('Traffic_Dataset.csv',index=False)
